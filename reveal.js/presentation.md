% Practical Steps for Increasing Openness and Reproducibility
% Ian Sullivan
% April 18, 2017


## Practical Steps for Increasing Openness and Reproducibility
Ian Sullivan    
April 18, 2017
<div class="text-small">Licensed [CC-BY](https://creativecommons.org/licenses/by/4.0/)</div>

<div class="notes">Hi everyone, thanks for joining today's webinar on practical steps for increasing openness and reproducibility in the sciences. I'm Ian Sullivan and I'll be leading this session.
</div>


## Welcome!
<div class="fragment">Presentation live at http://churchkey.org/cos/presentation.html</div>
<div class="fragment"><a href="mailto:ian@churchkey.org">ian@churchkey.org</a></div>

<div class="notes">This presentation is available live on the web right now. Source materials are available on my github account, which I'll have a link for at the end of the presentation..
...    
...    
If you take one thing away from this presentation I hope it is my email address, ian@churchkey.org.  Should you want to find out more about this topic or discuss any of the items we are going to cover today beyond the QA session, please reach out to me.</div>
## Why Open?
> * Scientific ideal
> * Practical benefits
> * Requirements

<div class="notes">
Many of you are likely interested in openness for different reasons.  The three most common ones I run into are...

* Openness and reproduciability are, after all, the foundation of science   
* The further your work spreads, the more people can cite and build upon it   
* Changing journal principles of openness and mandates from funders like NSF and the Gates Foundation for data management plans or complete open access

All of these are stirring interest in the topic.
</div>


## Movement towards the ideal
<div class="fragment">Appropriate steps vary by field</div>
<div class="fragment">Not all or nothing</div>

<div class="notes">
I think it is important to state that complete openness and reproduciability are ideals and, while we may all want to move towards those ideals, how we get there is going to differ...

Many fields face institutional and cultural obstacles that limit who can access work done in the field and what elements of that work are made public.    These run the gamut from pervasive journal paywalls in the most closed fields to concerns that publishing datasets in very open fields like high energy physics will lead to the need to debunk an army of armatures misinterpreting the vast sea of data produced by accelerator experiments. 

We should recognize that moving towards these ideals is a process and that all movement is progress...

My hope is that you will come away from this presentation with some concrete steps you can take to move towards greater openness and transparency in your work.
</div>


## Practical steps
<span class="highlight-green">Short</span>, <span class="highlight-yellow">medium</span>, and <span class="highlight-red">long</span> term.

<div class="notes">
Enough about the ideals then, let's talk about the movement. Each of the steps I'm outlining here is labeled short, medium, or long term to indicate the general level of effort and complexity required.  Any of the short term steps you should be able to implement after this presentation while medium and long term steps may require some planning and potentially reworking of your existing practices.

Depending on what field you are in some of these steps are going to be easier for you than others and some may already be routine. If that is the case, hopefully the slightly longer term steps are going to be more within your reach.

I've organized these steps loosely into ones that promote openness and ones that produce reproduciability though, of course, both are intertwined.
</div>


# Openness


## Open it on a phone
<div class="headline-short">[short]</div>
<div class="fragment">Easy hueristic to test availability and accessibility.</div>
<div class="fragment">Accessibility aids openness.</div>
<div class="fragment">Could you make a website?</div>

<div class="notes">
A quick first task is to try opening one of your own publications on your phone.

* I'm betting everyone here has read academic pdfs on their phones, or tried to. I certainly end up trying to zoom in and out of pdfs on my phone often enough....
* I find that opening your own work on a phone is a useful way to put yourself in the shoes of someone finding out about your work for the first time. What can they see, what connections are hard to make?
* If you are published in a paywalled journal, the phone is likely not even going to be able to get to your work. It can be easy to forget about some of the pay walls when running on our main machines underneath the umbrella of university-wide subscriptions. 
* If you can get to the pdf, test the limitations a bit.  How easy is it to navigate between sections? What about navigating from the document to any external links and back again? How would someone reading the pdf find your experimental data? ...
* Openness is about more than just access; accessibility matters too and can be the difference between someone diving into your work or getting frustrated and moving to the next item in their stack...
* Consider transforming articles into websites, especially if you are already using LaTeX since good automated tools exist for the task.
</div>


## Get a DOI
<div class="headline-short">[short]</div>
<div class="fragment">Make sure whatever data you publish will be locate-able for the long term.</div>

<div class="notes">
Another quick task is to get a DOI for your data...

DOI's should be free to you and they are durable, including mechanisms for changing locations if providers shut down. They are easy to include in your publications and should be common practice.  Being able to find your data is a vital part of being able to evaluate your research and DOIs provide a convenient way to connect that data to your published work.
</div>


## Publish in the raw
<div class="headline-medium">[medium]</div>
<div class="fragment">Make publishing as close to raw data part of your publication process and consider it during experiment design.</div>

<div class="notes">
* Of course, DOIs are only as useful as the data you publish and ideally that will be as close to the complete data you collect as possible...
* Publishing your data not only allows colleagues to verify your work it also enables them to look for additional connections beyond the scope of your experiment.
* Don't wait for colleagues to reach out post-publication and ask for the data ad-hoc.
* Depending on your field releasing raw or close to raw data will have privacy and anonymity implications so consult with your IRB where appropriate and consider ways to modify the scope of your collection so that more of the data you collect can be safely published.
</div>


## Pre-print
<div class="headline-long">[long]</div>

<div class="fragment text-big">**bp;dr**</div>
<div class="fragment">Norms vary</div>
<div class="fragment">Individual action *is* possible</div>

<div class="notes">
Who here has seen ... "bp;dr" before? Stands for "behind paywall, didn't read" and it is a nice shorthand for the "pay to access" model that is the biggest limitation on openness in scientific publishing today.

The single most effective way around these paywalls is to post a pre-publication copy of your work on a community pre-print server when you are submitting for publication.  Of course it may not be that simple for you...

If your field publishes pre-prints as a matter of course then you should already be doing this and you can shift your efforts at increased openness towards publishing data and other supporting materials that are not normally included in journals...

If your field is still shifting norms towards greater access, there *are* some steps you can take as an individual to move towards this ideal.
</div>


## Talk to your local librarians
<div class="headline-short">[short]</div>
<div class="fragment">Tell them you are interested in open access.</div>

> * Connect you with existing resources
> * Update you on changes in adjacent fields
> * Coordinate across campus

<div class="notes">
* Many librarians are passionate open access advocates for ideological reasons as well as because libraries pay the overwhelming bulk of journal subscription fees. They want to hear from you...
They can connect you to open access on campus including
...
...
...
* Resources may include campus open access portals and infrastructure, events, and opportunities for joint action across your institution.
</div>


## Publish supporting materials
<div class="headline-medium">[medium]</div>
<div class="fragment">>"The code, data structures, experimental design and parameters, documentation, and figures are all important for scholarship communication and result replication"

[The Legal Framework for Reproducible Scientific Research: Licensing and Copyright](http://ieeexplore.ieee.org/document/4720221/?reload=true)</div>

<div class="notes">In addition to the data you collect, there are many other elements of your work that are important to understanding your results but that are often unpublished.

Even if the journals in your field disallow pre-prints, you may be able to publish these supporting materials, which will still increase the openness and reproduciability of your piece as a whole.

....
This quote from the ieee gives some examples of the items you might publish and includes a link to useful licensing advice for these sort of supporting materials since you are often left on your own to select licensing terms for those materials.</div>


# Reproducible

## Technology not methodology
<div class="notes">
Reproduciability in science is a large topic, especially for a group from multiple disciplines like we have today.  The following steps are focused mostly on technological reproduciability rather than the methodological concerns, which are more likely to be field specific.
</div>

## Data Provenance.

Input -> <span class="text-big highlight-yellow">? ? ?</span> -> output

<div class="notes">
The big technological issue we will talk about is data provenance.  Where did your data come from and what did you do to it between collection and publishing?

Software and data processing have become fundamental components of modern science but current publishing only focuses on the output of that processing.  We have already talked about publishing the data we collect as input, now we will look at tools and steps that focus on documenting the intermediary transformation steps.  These will help maintain a good provenance for all the data in your research.
</div>

## Cite your software 
<div class="headline-short">[short]</div>
<div class="fragment">Include a list of the major software (with versions) you use in your methods section.</div>
<div class="fragment">Where possible, consider including links to those tools that are open source</div>

<div class="notes">Listing the names and version numbers of your software tools enables peers to more readily inspect your data, verify your calculations, and repeat your process...

Citing your software can also help you begin considering whether the software you depend upon is itself accessible and verifiable....

For those tools that *are* open and freely available, consider linking to them directly from your publication to encourage wider adoption.
</div>


## Prefer open tools
<div class="headline-medium">[medium]</div>
<div class="fragment">Consider:</div>
> * Julia
> * R
> * Python

<div class="fragment">If you can't audit the tool, how can you be sure of the results?</div>

<div class="notes">
Unlike equations, software programs are not guaranteed to return the same results year after year.  Anyone who has tried to open old Word documents in newer versions of the program has likely seen examples of this.

Free and open source software offer the best defense against these unforced errors by opening the tools themselves up to inspection by the whole community of scientists and engineers.

Changing the software you use for data collection and analysis can be a large task and I know that open options are not available for all jobs but very capable programs like ... ... Julia and R do exist along with the many high quality libraries for general purpose languages like ... Python....

Using free and open source software is a powerful way to improve the openness and reproduciability of your work.  If you cannot use open tools in the short term I strongly encourage you to select them in the future whenever you are looking to add new tools to your workflow.
</div>

## Workflow management tools
<div class="headline-medium">[medium]</div>
<div class="fragment">Special purpose tools for tracking and managing scientific data processing.</div>
<div class="fragment">Includes:</div>
> * [Apache Taverna](https://taverna.incubator.apache.org/)
> * [VisTrails](https://www.vistrails.org/index.php/Main_Page)
> * [Kepler](https://kepler-project.org/)
> * many others

<div class="notes">Workflow management tools are ...
...
...
...
...
...

These are powerful open source software tools that allow workflows to be completely documented and shared.  All are meant to become the central piece of software in your workflow and, as such, using one can require significant change to your current practices.  That change may or may not be warranted depending on the complexity of your needs and how well one of the existing tools fits them.

A paper comparing many of these tools is included in the reference section at the end of the presentation.

If you are primarily interested in the openness and reproduciability benefits of these tools there are some short term steps you can take to get many of those benefits regardless of the tools you currently use.
</div>


## Control your Versions
<div class="headline-short">[short]</div>
<div class="fragment">Version control tools like [git](https://git-scm.com/) and [svn](https://subversion.apache.org/) help in documenting the provenance of your data.</div>
<div class="fragment">(And maintaining your own sanity.)</div>
<div class="fragment">Good version management simplifies documentation.</div>

<div class="notes">
Few single tools can have as big an impact on improving your data management and workflow replicatability as version control...

They can also help save you hours spent looking through old versions of important files attached to seemingly endless email chains...

If you are not using version control yet, I strongly recommend it. If you are already using version control, consider adopting some shared conventions with your collaborators about how to track and describe changes... A piece with good suggestions for how to accomplish this using git commit messages is included in the references at the end.

Time invested in this kind of proactive documentation pays double dividends. It keeps things working during research and, by making your work slightly more self-documenting, it can ease the burden of pre-publication cleanup.  This cleanup burden can be a real obstacle, even for the most dedicated transparency advocate, so workflows that reduce it are precious.
</div>


## Re-run on a clean machine
<div class="headline-short">[short]</div>
<div class="fragment">Make sure you have documented all steps for processing your data by rerunning your analysis on a fresh machine.</div>

<div class="notes">Regardless of what tools you use, re-running your analysis on a clean machine can be a useful way to verify that you have full documentation for the various steps in your data processing...

If your analysis tools are free software you can use a spare thumb drive to install clean copies of the whole operating system and processing tools for this step.

This kind of verification is most useful for identifying the effect of any changed settings or other customizations to your work environment that might creep in over time.  It is much less likely to identify anything for virtual machines or machine clusters since these are generally set up in less idiosyncratic ways than our personal work stations.
</div>


## Pre-register: Registered Reports
<div class="headline-long">[long]</div>
<div class="fragment">Peer review of your experimental plan before the experimenting. 
<div class="notes">
Registered reports are the one methodological step I am suggesting and, like pre-prints, they are a powerful step towards openness and reproduciability for any field. ... 

Involving peer review at the design and preparation stage of an experiment is a powerful way to ensure you only spend your time running the highest quality experiments that are most likely to then be published.

Pre-registering is also a great way to make sure all the supplemental information about your experiment design and methodology is published while reducing the temptation to cherry pick through data at the end of an experiment.

Not all journals currently offer these kind of registered reports but this is a long term goal worth working towards.  If registered reports are not yet offered by the journals in your field, I hope you will follow their evolution in neighboring fields with interest. 
</div>

# A Place to start

## Maximum effect, minimum disruption

<div class="notes">There are many tools available to help you take some of the steps we've looked at here and  I encourage you to explore some of them to find the ones that best fit your needs.  Rather than run though a list of tools to accomplish each of the points we've looked at, lets take a minute at look at one tool that is designed to add as much openness and repoduciability to your work as possible while minimally disrupting your existing work flow.  If I have convinced you to try something new and you are looking for a single place to start, this is it.
</div>


## Open Science Framework
<div class="headline-medium">[medium]</div>
<div class="fragment">The [Open Science Framework](https://osf.io) is a collaboration and scientific project management tool offered for free by the [Center for Open Science](https://cos.io).
</div>

<div class="notes">
...
You can find information on the site about the range of features and services OSF offers but I want to highlight how using OSF can help you quickly take a number of the steps we have looked at here.
</div>


## OSF gets you
> * Easy publishing of supporting materials
> * A data management plan: 50+ years guaranteed
> * Automatic version control for files in the system
> * Integration with external storage
> * DOIs for your data (regardless of where you store it)

<div class="notes">
Those capabilities should be extra compelling because

...
...
...
...
...

* payed for by $250,000 preservation fund
* help document the provenance of your data
* (Google Drive, Dropbox, AWS, etc) so you do not need to migrate data or collaboration workflows already in place
</div>

## OSF is 
> * Free
> * Compatible with your existing data storage and reference management tools
> * Built by people who care about open and reproducible science

<div class="fragment">https://osf.io</div>

<div class="notes">
...
...
...
...
</div>

# References

## Software used
* [pandoc](http://pandoc.org/)
* [reveal.js](http://lab.hakim.se/reveal-js/#/)
* [git](https://git-scm.com/)

<div class="notes">Going to move through these pretty quickly so we can get to the QA. Again, this presentation and all the source materials are available online and I will put the address up at the end.

Software I used to build this presentation includes pandoc, reveal.js, and git. These are wonderful pieces of software that I encourage others to investigate.</div>

## Works cited
* [The Legal Framework for Reproducible Scientific Research: Licensing and Copyright](http://ieeexplore.ieee.org/document/4720221/?reload=true)
* [How to Write a Git commit message](https://chris.beams.io/posts/git-commit/)
* [Scientific Workflow: A Survey and Research Directions. (Barker A., van Hemert J. (2008))](http://www.adambarker.org/papers/ppam08.pdf)[pdf]

## Additional courses
>* [OSF (Open Science Framework) 101](https://youtu.be/VGpGUqSTKTw)
>* [Licensing your Research](https://youtu.be/NC7oquMgNdM) 
>* An Introduction to Open Source Scientific computing [Forthcoming] <!-- Course surveying major open source scientific computing projects by field and connecting students to resources on the use of general purpose free software that they may need for setting up and running their working environment-->
>* Designing for open data in a world of de-anonymization [Forthcoming] <!-- Course for social scientists on data collection considerations to help maximize the amount of data published while minimizing the impact on the privacy and anonymity of study participants, especially in light of de-anonymization research-->
>* Verification as a teaching tool [Forthcoming] <!-- Course on how to use the examination, and hopeful, validation of openly published studies to introduce students to data analyses tools and field-specific software-->

<div class="notes">
If you are interested in learning more about increasing the openness and reproducability of your work beyond this general overview, you may be interested in the following courses.
...
...
...
...
...
More details about these courses, especially the forthcoming ones, are available in the presentation file or the page source should anyone want to find out more.
</div>

## Thank you!

Ian Sullivan    
<a href="mailto:ian@churchkey.org">ian@churchkey.org</a>

<div class="fragment">Presentaion sources:

<a href="https://github.com/wikey/cos-practical-steps">github.com/wikey/cos-practical-steps</a></div> 


<div class="text-small">Licensed [CC-BY](https://creativecommons.org/licenses/by/4.0/)</div>
